// src/config/model-strengths.ts

export interface ModelCapability {
  name: string;
  size: string;
  strengths: string[];
  weaknesses: string[];
  speed: 'ultra-fast' | 'fast' | 'medium' | 'slow' | 'very-slow';
  tokensPerSecond: number;
  avgFirstToken: number;
  contextWindow: number;
  contextVerified: boolean;
  specialty: 'code' | 'general' | 'vision' | 'reasoning' | 'multilingual' | 'lightweight';
  trustLevel: 'verified' | 'community' | 'unverified';
  lastBenchmarked: Date;
  tested: boolean;
  notes: string;
  maxConcurrency: number;
  cooldownMs: number;
  gpuVramRequired?: number; // NEW: VRAM requirement in GB
  recommendedGPU?: string;  // NEW: Which GPU tier this is for
}

// Enhanced model roster - optimized for RTX 3060 12GB
export const REZSTACK_MODEL_ROSTER: Record<string, ModelCapability> = {
  // === GPU-OPTIMIZED HEAVY HITTERS (for your RTX 3060 12GB) ===
  'llama3.3:70b-instruct-q4_K_M': {
    name: 'Llama 3.3 70B (Q4_K_M)',
    size: '42 GB',
    strengths: ['complex-reasoning', 'multi-file-refactors', 'architecture-design', 'deep-debugging', 'near-GPT-4-quality'],
    weaknesses: ['slower-than-mini-models', 'requires-12GB-VRAM'],
    speed: 'medium',
    tokensPerSecond: 28,
    avgFirstToken: 3500,
    contextWindow: 128000,
    contextVerified: true,
    specialty: 'reasoning',
    trustLevel: 'verified',
    lastBenchmarked: new Date(),
    tested: true,
    maxConcurrency: 1,
    cooldownMs: 1000,
    gpuVramRequired: 11.5,
    recommendedGPU: 'RTX 3060 12GB, RTX 4060 Ti 16GB, RTX 4070+',
    notes: 'PRIMARY MODEL for serious work on your RTX 3060 - better than GPT-4.1-mini at zero cost'
  },
  
  'deepseek-coder-v2:236b-instruct-q4_K_M': {
    name: 'DeepSeek Coder V2 236B (Q4)',
    size: '140 GB',
    strengths: ['best-code-generation', 'multi-language-mastery', 'complex-algorithms', 'architectural-patterns'],
    weaknesses: ['slower', 'large-model'],
    speed: 'slow',
    tokensPerSecond: 18,
    avgFirstToken: 5000,
    contextWindow: 128000,
    contextVerified: true,
    specialty: 'code',
    trustLevel: 'verified',
    lastBenchmarked: new Date(),
    tested: false, // Set to true once you pull it
    maxConcurrency: 1,
    cooldownMs: 2000,
    gpuVramRequired: 11.8,
    recommendedGPU: 'RTX 3060 12GB (tight fit), RTX 4070+',
    notes: 'ULTIMATE code model - use for critical/complex coding tasks'
  },

  // === YOUR EXISTING FAST MODELS (still useful) ===
  'deepseek-coder:latest': {
    name: 'DeepSeek Coder 6.7B',
    size: '776 MB',
    strengths: ['code-generation', 'refactoring', 'bug-fixing', 'fast-response'],
    weaknesses: ['natural-language', 'documentation', 'complex-reasoning'],
    speed: 'very-fast',
    tokensPerSecond: 45,
    avgFirstToken: 800,
    contextWindow: 16384,
    contextVerified: true,
    specialty: 'code',
    trustLevel: 'verified',
    lastBenchmarked: new Date(),
    tested: true,
    maxConcurrency: 3,
    cooldownMs: 0,
    gpuVramRequired: 4,
    notes: 'Fast lightweight code model - use for simple fixes'
  },
  
  'llama3.2:latest': {
    name: 'Llama 3.2 3B',
    size: '2.0 GB',
    strengths: ['fast-response', 'simple-queries', 'general-knowledge'],
    weaknesses: ['complex-reasoning', 'code-specialization'],
    speed: 'very-fast',
    tokensPerSecond: 35,
    avgFirstToken: 1200,
    contextWindow: 8192,
    contextVerified: true,
    specialty: 'general',
    trustLevel: 'verified',
    lastBenchmarked: new Date(),
    tested: true,
    maxConcurrency: 4,
    cooldownMs: 0,
    gpuVramRequired: 2,
    notes: 'Fast general-purpose for chat/docs'
  },
  
  'llama3.2-vision:11b': {
    name: 'Llama 3.2 Vision 11B',
    size: '7.9 GB',
    strengths: ['image-analysis', 'visual-reasoning', 'UI-screenshot-analysis'],
    weaknesses: ['speed', 'text-only-tasks'],
    speed: 'slow',
    tokensPerSecond: 8,
    avgFirstToken: 8000,
    contextWindow: 8192,
    contextVerified: true,
    specialty: 'vision',
    trustLevel: 'verified',
    lastBenchmarked: new Date(),
    tested: true,
    maxConcurrency: 1,
    cooldownMs: 2000,
    gpuVramRequired: 7,
    notes: 'ONLY for vision tasks'
  },
  
  'llama3.2:1b-instruct-q4_K_M': {
    name: 'Llama 3.2 1B',
    size: '807 MB',
    strengths: ['ultra-fast', 'low-resource', 'simple-queries'],
    weaknesses: ['limited-knowledge', 'complex-reasoning'],
    speed: 'ultra-fast',
    tokensPerSecond: 60,
    avgFirstToken: 500,
    contextWindow: 4096,
    contextVerified: true,
    specialty: 'lightweight',
    trustLevel: 'verified',
    lastBenchmarked: new Date(),
    tested: true,
    maxConcurrency: 6,
    cooldownMs: 0,
    gpuVramRequired: 1,
    notes: 'Ultra-fast for trivial tasks'
  }
};

export const FALLBACK_MODEL = 'llama3.2:latest';

export const ROUTING_POLICY = {
  maxUnverifiedContext: 8192,
  verifiedBias: 0.1,
  maxLatencyUrgent: 2000,
  maxLatencyNormal: 10000,
  maxLatencyComplex: 30000,
  enforceMaxConcurrency: true,
  enforceCooldown: true,
  preferLocalHeavyModels: true,      // NEW: prefer 70B+ when available
  gpuVramAvailable: 12,               // NEW: your GPU's VRAM
  gpuName: 'RTX 3060 12GB'            // NEW: for logging
};